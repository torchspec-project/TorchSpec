# Configuration for train_entry.py with SglEngine inference (nested config format)
#
# GPU allocation:
#   - 2 GPUs for inference (duplicate mode: each engine has full model copy)
#   - 2 GPUs for training (DP/FSDP: model sharded across 2 GPUs)
#   - Total: 4 GPUs
#
# Usage:
#   python -m torchspec.train_entry --config configs/sglang_qwen3_8b.yaml
#   ./examples/qwen3-8b-single-node/run.sh

model:
  target_model_path: Qwen/Qwen3-8B
  trust_remote_code: true

dataset:
  train_data_path: ./examples/data/sample_conversations.jsonl
  eval_data_path: ./examples/data/eval_conversations.jsonl
  eval_interval: 100
  chat_template: qwen
  prompt_key: conversations

training:
  attention_backend: flex_attention
  micro_batch_size: 1
  draft_accumulation_steps: 1
  learning_rate: 1e-4
  max_concurrent_batches: 1
  max_grad_norm: 0.5
  max_seq_length: 16384
  num_epochs: 1
  seed: 42
  training_num_gpus_per_node: 2
  training_num_nodes: 1
  ttt_length: 7
  save_per_epoch: true
  warmup_ratio: 0.015

inference:
  inference_engine_type: sgl  # Use SglEngine instead of HFEngine
  inference_num_gpus: 1
  inference_num_gpus_per_engine: 1
  inference_num_gpus_per_node: 1
  max_sample_pool_size: 64       # Max samples in controller pool
  inference_buffer_threshold: 32     # Fetch prompts when buffer < threshold
  inference_batch_size: 8
  sglang:
    tp_size: 1
    mem_fraction_static: 0.7

mooncake:
  master_server_address: null
  metadata_server: null
  protocol: tcp
  global_segment_size: 16GB
  local_buffer_size: 4GB

output_dir: ./outputs/qwen3-8b-single-node
cache_dir: ./cache
model_download_dir: null

debug:
  save_debug_train_data: null
  debug_train_only: false
  debug_inference_only: false

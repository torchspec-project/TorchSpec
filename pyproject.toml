[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "torchspec"
dynamic = ["version", "description"]
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "transformers==4.57.1",
    "datasets",
    "setuptools",
    "tqdm",
    "wandb",
    "qwen-vl-utils",
    "psutil",
    "numpy<2.4",
    "accelerate",
    "pydantic",
    "openai-harmony",
    "cmake",
    "ninja",
    "packaging",
    "pyzmq",
    "mooncake-transfer-engine",
    "openai",
    "omegaconf",
    "ray",
    "sglang-router",
    "numba",
]

[tool.setuptools]
packages = ["torchspec"]

[project.optional-dependencies]
dev = [
    "pre-commit",
    "pytest",
    "ruff",
]

fa = [
    "flash-attn-cute @ git+https://github.com/Dao-AILab/flash-attention.git@fec3a6a18460c1b40f097208d4c16fe8964a679d#subdirectory=flash_attn/cute",
    "nvidia-cutlass-dsl==4.4.0.dev1",
    "nvidia-cutlass-dsl-libs-base==4.4.0.dev1",
]


[tool.setuptools.dynamic]
version = {file = "version.txt"}
description = {file = "README.md"}

[tool.ruff]
line-length = 100
target-version = "py312"

[tool.ruff.lint]
select = ["E", "F", "I", "W", "TID252"]
ignore = ["E501"]

[tool.ruff.lint.flake8-tidy-imports]
ban-relative-imports = "all"

[tool.ruff.lint.isort]
known-first-party = ["torchspec"]

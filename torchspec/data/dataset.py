# Copyright (c) 2026 LightSeek Foundation
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import hashlib
import logging as _logging
import multiprocessing as mp
import os

import torch
from tqdm import tqdm

from torchspec.data.preprocessing import _normalize_conversation
from torchspec.data.template import TEMPLATE_REGISTRY
from torchspec.data.utils import (
    estimate_row_count,
    extract_media_urls,
    flatten_multimodal_content,
    load_hf_dataset,
)
from torchspec.utils.logging import logger

_logging.getLogger("transformers_modules").setLevel(_logging.ERROR)

_worker_state = {}


def _init_tokenize_worker(
    tokenizer_path, trust_remote_code, chat_template_name, last_turn_loss_only=False
):
    """Initializer for each worker process — loads tokenizer once."""
    from torchspec.data.preprocessing import preprocess_conversations
    from torchspec.utils.processing import load_tokenizer

    _logging.getLogger("transformers_modules").setLevel(_logging.ERROR)
    _worker_state["tokenizer"] = load_tokenizer(tokenizer_path, trust_remote_code=trust_remote_code)
    _worker_state["template"] = TEMPLATE_REGISTRY.get(chat_template_name)
    _worker_state["preprocess"] = preprocess_conversations
    _worker_state["last_turn_loss_only"] = last_turn_loss_only


def _tokenize_single(args):
    """Worker function — tokenize one sample."""
    messages, max_length = args
    processed = _worker_state["preprocess"](
        _worker_state["tokenizer"],
        [messages],
        _worker_state["template"],
        max_length=max_length,
        is_preformatted=False,
        include_attention_mask=False,
        use_packed_loss_mask=True,
        return_formatted_text=True,
        last_turn_loss_only=_worker_state.get("last_turn_loss_only", False),
    )
    if not processed["input_ids"]:
        return None
    # Return plain lists instead of tensors to avoid shared memory mmap
    # exhaustion when transferring results across process boundaries.
    input_ids = processed["input_ids"][0]
    return {
        "input_ids": input_ids.tolist() if hasattr(input_ids, "tolist") else input_ids,
        "packed_loss_mask": processed["packed_loss_mask"][0],
        "formatted_prompt": processed["formatted_text"][0],
    }


def load_conversation_dataset(args):
    """Load conversation dataset and tokenize with proper loss_mask for training.

    Returns list of dicts with input_ids/loss_mask.
    Requires chat_template to be set. Uses preprocess_conversations for proper loss masking.
    Caches tokenized results for efficient reloading.
    """
    prompt_key = getattr(args, "prompt_key", "text")
    chat_template_name = getattr(args, "chat_template", None)
    max_length = args.max_seq_length
    logger.info(f"Max sequence length allowed for training: {max_length}")

    if not chat_template_name:
        raise ValueError("chat_template must be set for load_conversation_dataset")

    custom_template = TEMPLATE_REGISTRY.get(chat_template_name)
    hf_dataset = load_hf_dataset(args.train_data_path)

    dataset_name = os.path.basename(args.train_data_path)
    file_stat = ""
    if os.path.isfile(args.train_data_path):
        st = os.stat(args.train_data_path)
        file_stat = f"-{st.st_size}-{st.st_mtime}"
    last_turn_loss_only_flag = getattr(args, "last_turn_loss_only", False)
    cache_params = f"{dataset_name}-{args.train_data_path}{file_stat}-{args.target_model_path}-{max_length}-{chat_template_name}-ltlo={last_turn_loss_only_flag}"
    cache_key = hashlib.md5(cache_params.encode()).hexdigest()
    cache_dir = os.path.join(getattr(args, "cache_dir", "./cache"), "tokenized_dataset")
    cache_path = os.path.join(cache_dir, f"{cache_key}.pt")

    if os.path.exists(cache_path):
        logger.info(f"Loading tokenized dataset from cache: {cache_path}")
        prompts = torch.load(cache_path, weights_only=False)
        logger.info(f"Loaded {len(prompts)} cached tokenized samples")
        return prompts

    logger.info(f"Tokenizing dataset (cache will be saved to {cache_path})")

    total_estimate = estimate_row_count(args.train_data_path)
    num_proc = getattr(args, "num_proc", 64)

    # Pass 1: collect and normalize raw samples (fast I/O, no tokenization)
    raw_samples = []
    for idx, sample in enumerate(tqdm(hf_dataset, desc="Loading samples", total=total_estimate)):
        raw_prompt = sample.get(prompt_key, "")

        if not isinstance(raw_prompt, list):
            raise ValueError(
                f"Expected conversation format (list of messages) for sample {idx}, got {type(raw_prompt)}"
            )

        messages = _normalize_conversation(raw_prompt)
        multimodal_inputs = extract_media_urls(messages)
        flatten_multimodal_content(messages, custom_template.image_placeholder)
        data_id = sample.get("id", f"sample_{idx}")
        raw_samples.append((data_id, messages, multimodal_inputs))

    logger.info(f"Loaded {len(raw_samples)} samples, tokenizing with {num_proc} workers...")

    # Pass 2: tokenize in parallel
    work_items = [(messages, max_length) for _, messages, _ in raw_samples]

    last_turn_loss_only = getattr(args, "last_turn_loss_only", False)
    if last_turn_loss_only:
        logger.info("last_turn_loss_only=True: loss mask will only cover the last assistant turn")

    if num_proc <= 1:
        _init_tokenize_worker(args.target_model_path, True, chat_template_name, last_turn_loss_only)
        results = [_tokenize_single(item) for item in tqdm(work_items, desc="Tokenizing dataset")]
    else:
        with mp.Pool(
            num_proc,
            initializer=_init_tokenize_worker,
            initargs=(args.target_model_path, True, chat_template_name, last_turn_loss_only),
        ) as pool:
            results = list(
                tqdm(
                    pool.imap(_tokenize_single, work_items, chunksize=64),
                    total=len(work_items),
                    desc="Tokenizing dataset",
                )
            )

    # Collect results
    prompts = []
    skipped = 0
    for (data_id, _, multimodal_inputs), result in zip(raw_samples, results):
        if result is None:
            skipped += 1
            continue
        input_ids = result["input_ids"]
        if isinstance(input_ids, list):
            input_ids = torch.tensor(input_ids)
        entry = {
            "data_id": data_id,
            "prompt": None,
            "metadata": {},
            "multimodal_inputs": multimodal_inputs,
            "input_ids": input_ids,
            "packed_loss_mask": result["packed_loss_mask"],
            "formatted_prompt": result["formatted_prompt"],
        }
        prompts.append(entry)

    if skipped:
        logger.warning(f"Skipped {skipped} samples (empty source or zero loss mask)")

    os.makedirs(cache_dir, exist_ok=True)
    torch.save(prompts, cache_path)
    logger.info(f"Saved {len(prompts)} tokenized samples to cache: {cache_path}")

    return prompts
